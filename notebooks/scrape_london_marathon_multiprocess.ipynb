{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting up required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import concurrent.futures  # to allow multithreading\n",
    "from bs4 import BeautifulSoup, SoupStrainer  # navigate through web pages\n",
    "from typing import Optional"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Function Definitions:\r\n",
    "Two main functions, get_results old and new. These correspond to the two website styles. Also functions to generate urls, and wrapper function to be applied with map (this enables the multiprocessing/multithreaded approach). The two get_results functions have been superceded by a single function that checks years and sets values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_results_table(url: str, sex: str, year: int) -> pd.DataFrame:\n",
    "    \"\"\"Scrape london marathon\"\"\"\n",
    "\n",
    "    # Set parsing values for different years (different page layouts)\n",
    "    if year >= 2019:\n",
    "        strainer_expr = 'class_ = \"section-main\"'\n",
    "        row_expr = 'class_ = \"list-group-item\"'\n",
    "        cell_expr = 'class_ = \"list-field\"'\n",
    "    else:\n",
    "        strainer_expr = '\"tbody\"'\n",
    "        row_expr = '\"tr\"'\n",
    "        cell_expr = '\"td\"'\n",
    "    if year == 2021:\n",
    "        row_indexes = [0, 1, 2, 3, 4, 5, 6, 9]\n",
    "    elif year == 2014:\n",
    "        row_indexes = [0, 1, 2, 3, 5, 6, 7, 9]\n",
    "    else:\n",
    "        row_indexes = [0, 1, 2, 3, 4, 5, 6, 8]\n",
    "    site = requests.get(url).text\n",
    "    # Soup strainer restricts content to speed up soup\n",
    "    strainer = SoupStrainer(eval(strainer_expr))\n",
    "\n",
    "    soup = BeautifulSoup(site, \"lxml\", parse_only=strainer)\n",
    "\n",
    "    # Loop through each row and column to create a list of cells\n",
    "    my_table = []\n",
    "    for row in soup.find_all(eval(row_expr)):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(eval(cell_expr)):\n",
    "            alt_text = cell.find(\"span\")\n",
    "            if alt_text is not None:\n",
    "                cell = alt_text[\"title\"]\n",
    "            else:\n",
    "                cell = cell.text\n",
    "            row_data.append(cell)\n",
    "        # If the row isn't empty, then create a dict of the row to create dataframe from\n",
    "        if row_data:\n",
    "            data_item = {\n",
    "                \"Place (Overall)\": row_data[row_indexes[0]],\n",
    "                \"Place (Gender)\": row_data[row_indexes[1]],\n",
    "                \"Place (Category)\": row_data[row_indexes[2]],\n",
    "                \"Name\": row_data[row_indexes[3]],\n",
    "                \"Sex\": sex,\n",
    "                \"Club\": row_data[row_indexes[4]],\n",
    "                \"Running Number\": row_data[row_indexes[5]],\n",
    "                \"Category\": row_data[row_indexes[6]],\n",
    "                \"Finish\": row_data[row_indexes[7]],\n",
    "                \"Year\": year,\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "    results = pd.DataFrame(my_table).iloc[1:]  # Strip table header\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results_new(url: str, sex: str, year: int) -> pd.DataFrame:\n",
    "    \"\"\"Function to scrape modern virgin london marathon results page (2019 to 2021)\"\"\"\n",
    "\n",
    "    site = requests.get(url).text\n",
    "    # Soup strainer restricts content to speed up soup\n",
    "    strainer = SoupStrainer(class_=\"section-main\")\n",
    "    soup = BeautifulSoup(site, \"lxml\", parse_only=strainer)\n",
    "    # fields = soup.find(class_='section-main')\n",
    "\n",
    "    # Loop through each row and column to create a list of cells\n",
    "    my_table = []\n",
    "    for row in soup.find_all(class_=\"list-group-item\"):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(class_=\"list-field\"):\n",
    "            row_data.append(cell.text)\n",
    "        # If the row isn't empty, then create a dict of the row to create dataframe from\n",
    "        if row_data:\n",
    "            data_item = {\n",
    "                \"Place (Overall)\": row_data[0],\n",
    "                \"Place (Gender)\": row_data[1],\n",
    "                \"Place (Category)\": row_data[2],\n",
    "                \"Name\": row_data[3],\n",
    "                \"Sex\": sex,\n",
    "                \"Club\": row_data[4],\n",
    "                \"Running Number\": row_data[5],\n",
    "                \"Category\": row_data[6],\n",
    "                \"Finish\": row_data[8],\n",
    "                \"Year\": year,\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "    results = pd.DataFrame(my_table).iloc[1:]  # Strip table header\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results_old(url: str, sex: str, year: int) -> pd.DataFrame:\n",
    "    \"\"\"Function to scrape old virgin london marathon results page (2014 to 2018)\"\"\"\n",
    "\n",
    "    site = requests.get(url).text  # Use requests to get content from site\n",
    "    strainer = SoupStrainer(\"tbody\")  # Soup strainer restricts content to sped up soup\n",
    "    soup = BeautifulSoup(site, \"lxml\", parse_only=strainer)  # Parse the html\n",
    "\n",
    "    my_table = []\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            # Check if cell has alt text, if so use that as data\n",
    "            alt_text = cell.find(\"span\")\n",
    "            if alt_text is not None:\n",
    "                cell = alt_text[\"title\"]\n",
    "            else:\n",
    "                cell = cell.text\n",
    "            row_data.append(cell)\n",
    "        # If the row isn't empty, then create a dict of the row to create dataframe from\n",
    "        if len(row_data) > 0 and year != 2014:\n",
    "            data_item = {\n",
    "                \"Place (Overall)\": row_data[0],\n",
    "                \"Place (Gender)\": row_data[1],\n",
    "                \"Place (Category)\": row_data[2],\n",
    "                \"Name\": row_data[3],\n",
    "                \"Sex\": sex,\n",
    "                \"Club\": row_data[4],\n",
    "                \"Running Number\": row_data[5],\n",
    "                \"Category\": row_data[6],\n",
    "                \"Finish\": row_data[8],\n",
    "                \"Year\": year,\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "        elif len(row_data) > 0 and year == 2014:\n",
    "            data_item = {\n",
    "                \"Place (Overall)\": row_data[0],\n",
    "                \"Place (Gender)\": row_data[1],\n",
    "                \"Place (Category)\": row_data[2],\n",
    "                \"Name\": row_data[3],\n",
    "                \"Sex\": sex,\n",
    "                \"Club\": row_data[5],\n",
    "                \"Running Number\": row_data[6],\n",
    "                \"Category\": row_data[7],\n",
    "                \"Finish\": row_data[9],\n",
    "                \"Year\": year,\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "    results = pd.DataFrame(my_table)  # Strip table header\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(url):\n",
    "    \"\"\"Function chooses what results func to apply. Used to allow single function for pool.map\"\"\"\n",
    "\n",
    "    # Get year and sex from the URL\n",
    "    year = int(re.search(r\"\\.com/(\\d{4})/\", url).group(1))\n",
    "    sex = re.search(r\"sex%5D=(\\w)\", url).group(1)\n",
    "    page = re.search(r\"page=(.*?)&event=\", url).group(1)\n",
    "    print(f\"Getting results for {sex} in {year}, page {page}\")\n",
    "    # if year >= 2019:\n",
    "    #     data = get_results_new(url, sex, year)\n",
    "    # elif year >= 2010:\n",
    "    #     data = get_results_old(url, sex, year)\n",
    "    # else:\n",
    "    #     data = None\n",
    "\n",
    "    data = get_results_table(url, sex, year)\n",
    "\n",
    "    print(f\"Finished getting results for {sex} in {year}, page {page}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_virgin_urls(sex, pages, year):\n",
    "    \"\"\"Get a list of urls, this is needed to be used to apply function to to then use multiprocessing\"\"\"\n",
    "\n",
    "    urls = [\"NaN\"] * pages\n",
    "    if year >= 2019:\n",
    "        for i in range(pages):\n",
    "            urls[i] = (\n",
    "                f\"https://results.virginmoneylondonmarathon.com/\"\n",
    "                + str(year)\n",
    "                + \"/?page=\"\n",
    "                + str(i + 1)\n",
    "                + \"&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=\"\n",
    "                + sex\n",
    "                + \"&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name\"\n",
    "            )\n",
    "    elif year >= 2014:\n",
    "        for i in range(pages):\n",
    "            urls[i] = (\n",
    "                \"https://results.virginmoneylondonmarathon.com/\"\n",
    "                + str(year)\n",
    "                + \"/?page=\"\n",
    "                + str(i + 1)\n",
    "                + \"&event=MAS&num_results=1000&pid=list&search%5Bage_class%5D=%25&search%5Bsex%5D=\"\n",
    "                + sex\n",
    "            )\n",
    "    elif year >= 2010:\n",
    "        for i in range(pages):\n",
    "            urls[i] = (\n",
    "                \"https://results.virginmoneylondonmarathon.com/\"\n",
    "                + str(year)\n",
    "                + \"/index.php?page=\"\n",
    "                + str(i + 1)\n",
    "                + \"&event=MAS&num_results=1000&pid=search&search%5Bsex%5D=\"\n",
    "                + sex\n",
    "            )\n",
    "    return urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Time to get to work\n",
    "\n",
    "Use get_virgin_urls with a list of page numbers (need to create loop for that) and a range of years to produce the list of urls that we will iterate over\n",
    "\n",
    "NOTE: Is possible to scrape 2010, but slightly diff. format so need to produce a different function for that."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get no. of pages using technique like\n",
    "# Not kept in/included in functions because requests take forever!\n",
    "# url1 = 'https://results.virginmoneylondonmarathon.com/2020/?page='\n",
    "# url2 = '&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D='\n",
    "# url3 = '&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "# site_m=requests.get(url1+'1'+url2+'M' +url3).text\n",
    "# site_w=requests.get(url1+'1'+url2+'W' +url3).text\n",
    "# soup_m = BeautifulSoup(site_m,'lxml')\n",
    "# soup_w = BeautifulSoup(site_w,'lxml')\n",
    "\n",
    "# m_pages = int(soup_m.find(class_='pages').text[-4:-2])\n",
    "# w_pages = int(soup_w.find(class_='pages').text[-4:-2])\n",
    "# print(m_pages, w_pages)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "urls = []\n",
    "years = None  # Set to list of years if just want some\n",
    "\n",
    "pages_men = {\n",
    "    2011: 23,\n",
    "    2012: 24,\n",
    "    2013: 23,\n",
    "    2014: 23,\n",
    "    2015: 24,\n",
    "    2016: 24,\n",
    "    2017: 24,\n",
    "    2018: 24,\n",
    "    2019: 29,\n",
    "    2020: 22,\n",
    "    2021: 25,\n",
    "}\n",
    "pages_women = {\n",
    "    2011: 13,\n",
    "    2012: 14,\n",
    "    2013: 13,\n",
    "    2014: 14,\n",
    "    2015: 15,\n",
    "    2016: 16,\n",
    "    2017: 16,\n",
    "    2018: 17,\n",
    "    2019: 21,\n",
    "    2020: 22,\n",
    "    2021: 17,\n",
    "}\n",
    "\n",
    "print(\"Generating URLS...\")\n",
    "# Too lazy to setup dataframe for years/pages/gender, need to\n",
    "# Check if years to search has been set\n",
    "if years is None:\n",
    "    years = [yr for yr in pages_men.keys() if yr != 2020]  # 2020 has disappeared?\n",
    "\n",
    "for year in years:\n",
    "    w_urls = generate_virgin_urls(\"W\", pages_women[year], year)\n",
    "    m_urls = generate_virgin_urls(\"M\", pages_men[year], year)\n",
    "    urls = urls + m_urls + w_urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell uses ```multiprocess.pool``` to divide the work of making requests and parsing between a number of worker processes. This currently doesn't lead to any appreciable improvement in speed, needs further investigation! Possibly need to investigate proper threading.\n",
    "\n",
    "This process requires an iterable and a function to apply it over."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "# #Setup multiprocessing and start scraping!\n",
    "# pool = Pool(8)\n",
    "# #Scrape multiprocessing\n",
    "# data = pool.map(get_results, urls)\n",
    "# #Cleanup after yourself\n",
    "# pool.terminate()\n",
    "# pool.join()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell does a similar process, but using multithreading instead of multiprocessing, via ```concurrent.futures.ThreadPoolExecutor```."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Warning: Takes ~10mins to complete!\n",
    "# Trying using multithreading instead of multiprocessing\n",
    "MAX_THREADS = 30\n",
    "threads = min(MAX_THREADS, len(urls))\n",
    "\n",
    "print(\"Beginning data extract....\")\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "    data = list(executor.map(get_results, urls))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get dataframe from list of df (sep cell to allow for recreation without re-parsing)\n",
    "results = pd.concat(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Some data cleaning\n",
    "# Remove leftover titles\n",
    "results[\"Club\"] = results[\"Club\"].str.replace(\"Club\", \"\", regex=False)\n",
    "results[\"Running Number\"] = results[\"Running Number\"].str.replace(\n",
    "    \"Running Number\", \"\", regex=False\n",
    ")\n",
    "results[\"Running Number\"] = results[\"Running Number\"].str.replace(\n",
    "    \"Runner Number\", \"\", regex=False\n",
    ")\n",
    "results[\"Category\"] = results[\"Category\"].str.replace(\"Category\", \"\", regex=False)\n",
    "results[\"Finish\"] = results[\"Finish\"].str.replace(\"Finish\", \"\", regex=False)\n",
    "\n",
    "# Extract country groups, like (USA), from Name group\n",
    "results[\"Country\"] = results[\"Name\"].str.extract(r\"(\\([A-Z]{3,}\\))\")\n",
    "# Remove brackets in country\n",
    "results[\"Country\"] = results[\"Country\"].str.replace(r\"\\(|\\)\", \"\", regex=True)\n",
    "# Remove country group from name column\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\([A-Z]{3}\\))\", \"\", regex=True)\n",
    "\n",
    "# Split first/lastname into new columns\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(»)\", \"\", regex=True)\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\n)\", \"\", regex=True)\n",
    "last_first = results[\"Name\"].str.split(pat=\",\", n=1, expand=True)\n",
    "results[\"FirstName\"], results[\"LastName\"] = last_first[1], last_first[0]\n",
    "# Remove comma from Name column, so that this can be saved as a CSV ----- Must happen after splitting Name into two cols!!\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\,)\", \"\", regex=True)\n",
    "\n",
    "# Change W to F for 2020\n",
    "results[\"Sex\"] = results[\"Sex\"].str.replace(\"W\", \"F\")\n",
    "\n",
    "# Create DSQ column for did not finish results, to avoid removing info when using nan\n",
    "results[\"DSQ\"] = results[\"Finish\"] == \"DSQ\"\n",
    "\n",
    "# Replace non-standard '–' with NaN for missing vals\n",
    "results = results.replace(\"DSQ\", np.nan)\n",
    "results = results.replace(\"–\", np.nan)\n",
    "results = results.replace(\"\", np.nan)\n",
    "\n",
    "# Delete odd race number row - gives fastest male/female so is duplicate\n",
    "results = results.loc[\n",
    "    (results[\"Running Number\"] != \"RM9999\") & (results[\"Running Number\"] != \"RF9999\")\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = results.astype(\n",
    "    {\n",
    "        \"Place (Overall)\": \"float64\",\n",
    "        \"Place (Gender)\": \"float64\",\n",
    "        \"Place (Category)\": \"float64\",\n",
    "        \"Name\": str,\n",
    "        \"Sex\": str,\n",
    "        \"Club\": str,\n",
    "        \"Running Number\": str,\n",
    "        \"Category\": \"category\",\n",
    "        \"Year\": \"Int64\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Due to an irritating bug with converting objects to Int64, needed to first convert to float and then to int\n",
    "results = results.astype(\n",
    "    {\n",
    "        \"Place (Overall)\": \"Int64\",\n",
    "        \"Place (Gender)\": \"Int64\",\n",
    "        \"Place (Category)\": \"Int64\",\n",
    "    }\n",
    ")\n",
    "results[\"Finish\"] = pd.to_timedelta(results[\"Finish\"])\n",
    "results[\"Finish (Total Seconds)\"] = results[\"Finish\"].dt.total_seconds()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's see what we've got\n",
    "results.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# And save them in a csv\n",
    "results.to_csv(\n",
    "    r\"C:\\Users\\michael.walshe\\Documents\\Python Projects\\scrape_london_marathon\\London_Marathon_Big.csv\",\n",
    "    index=False,\n",
    "    header=True,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Profiling function to find bottlenecks, need to speed up parser more???\n",
    "# url = 'https://results.virginmoneylondonmarathon.com/2019/?page=1&event=ALL&'+ \\\n",
    "#       'num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=M&search%5Bage_'+ \\\n",
    "#        'class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "# %lprun -f get_results_new get_results_new(url, \"M\", 2019)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('anc': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "c8b21321f26bdc6ac60d53849d9315e38be639ed399267d59045732bea44f11b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}