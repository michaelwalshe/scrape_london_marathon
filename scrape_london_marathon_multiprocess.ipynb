{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting up required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import lxml  # alternative xml parser for bs4\n",
    "import cchardet  # c processing of characters, faster?\n",
    "import re  # regex\n",
    "import concurrent.futures  # to allow multithreading\n",
    "from bs4 import BeautifulSoup, SoupStrainer  # navigate through web pages\n",
    "from multiprocessing import Pool, cpu_count  # to allow multiprocessing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Function Definitions:\n",
    "Two main functions, get_results old and new. These correspond to the two website styles. Also functions to generate urls, and wrapper function to be applied with map (this enables the multiprocessing/multithreaded approach)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_results_new(url, sex, year):\n",
    "    # Function to scrape modern virgin london marathon results page (2020 and 2019)\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    site = requests.get(url).text  # Use requests to get content from site\n",
    "    strainer = SoupStrainer(\n",
    "        class_=\"section-main\"\n",
    "    )  # Soup strainer restricts content to sped up soup\n",
    "    soup = BeautifulSoup(site, \"lxml\", parse_only=strainer)  # Parse the html\n",
    "    # fields = soup.find(class_='section-main')\n",
    "\n",
    "    # Loop through each row and column to create a list of cells\n",
    "    my_table = []\n",
    "    for row in soup.find_all(class_=\"list-group-item\"):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(class_=\"list-field\"):\n",
    "            row_data.append(cell.text)\n",
    "        # If the row isn't empty, then create a dict of the row to create dataframe from\n",
    "        # If 2020, then use different row index for Finish\n",
    "        if len(row_data) > 0 and year != 2020:\n",
    "            data_item = {\n",
    "                \"Place (Overall)\": row_data[0],\n",
    "                \"Place (Gender)\": row_data[1],\n",
    "                \"Place (Category)\": row_data[2],\n",
    "                \"Name\": row_data[3],\n",
    "                \"Sex\": sex,\n",
    "                \"Club\": row_data[4],\n",
    "                \"Running Number\": row_data[5],\n",
    "                \"Category\": row_data[6],\n",
    "                \"Finish\": row_data[7],\n",
    "                \"Year\": year,\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "        elif len(row_data) > 0 and year == 2020:\n",
    "            data_item = {\n",
    "                \"Place (Overall)\": row_data[0],\n",
    "                \"Place (Gender)\": row_data[1],\n",
    "                \"Place (Category)\": row_data[2],\n",
    "                \"Name\": row_data[3],\n",
    "                \"Sex\": sex,\n",
    "                \"Club\": row_data[4],\n",
    "                \"Running Number\": row_data[5],\n",
    "                \"Category\": row_data[6],\n",
    "                \"Finish\": row_data[8],\n",
    "                \"Year\": year,\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "    df = pd.DataFrame(my_table).iloc[1:]  # Strip table header\n",
    "    results = results.append(df)  # Append to results\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_results_old(url, sex, year):\n",
    "    # Function to scrape old virgin london marathon results page (2014 to 2018)\n",
    "\n",
    "    results = pd.DataFrame()  # Set up empty dataframe for results\n",
    "\n",
    "    site = requests.get(url).text  # Use requests to get content from site\n",
    "    strainer = SoupStrainer(\"tbody\")  # Soup strainer restricts content to sped up soup\n",
    "    soup = BeautifulSoup(site, \"lxml\", parse_only=strainer)  # Parse the html\n",
    "\n",
    "    my_table = []\n",
    "    for row in soup.find_all(\"tr\"):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            # Check if cell has alt text, if so use that as data\n",
    "            alt_text = cell.find(\"span\")\n",
    "            if alt_text != None:\n",
    "                cell = alt_text[\"title\"]\n",
    "            else:\n",
    "                cell = cell.text\n",
    "            row_data.append(cell)\n",
    "        # If the row isn't empty, then create a dict of the row to create datafram from\n",
    "        if len(row_data) > 0 and year != 2014:\n",
    "            data_item = {\n",
    "                \"Place (Overall)\": row_data[0],\n",
    "                \"Place (Gender)\": row_data[1],\n",
    "                \"Place (Category)\": row_data[2],\n",
    "                \"Name\": row_data[3],\n",
    "                \"Sex\": sex,\n",
    "                \"Club\": row_data[4],\n",
    "                \"Running Number\": row_data[5],\n",
    "                \"Category\": row_data[6],\n",
    "                \"Finish\": row_data[8],\n",
    "                \"Year\": year,\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "        elif len(row_data) > 0 and year == 2014:\n",
    "            data_item = {\n",
    "                \"Place (Overall)\": row_data[0],\n",
    "                \"Place (Gender)\": row_data[1],\n",
    "                \"Place (Category)\": row_data[2],\n",
    "                \"Name\": row_data[3],\n",
    "                \"Sex\": sex,\n",
    "                \"Club\": row_data[5],\n",
    "                \"Running Number\": row_data[6],\n",
    "                \"Category\": row_data[7],\n",
    "                \"Finish\": row_data[9],\n",
    "                \"Year\": year,\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "    df = pd.DataFrame(my_table)  # Strip table header\n",
    "    results = results.append(df)  # Append to results\n",
    "\n",
    "    return results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_results(url):\n",
    "    # Function choose what results func to apply\n",
    "    # Used to allow single function for pool.map\n",
    "    year = int(\n",
    "        re.search(\"\\.com\\/(\\d{4})\\/\", url).group(1)\n",
    "    )  # Check what year the url is\n",
    "    sex = re.search(\"sex%5D=(\\w)\", url).group(1)\n",
    "    if year >= 2019:\n",
    "        data = get_results_new(url, sex, year)\n",
    "    elif year >= 2010:\n",
    "        data = get_results_old(url, sex, year)\n",
    "    else:\n",
    "        data = None\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_virgin_urls(sex, pages, year):\n",
    "    # Get a list of urls, this is needed to be used to apply function to to then use multiprocessing\n",
    "    urls = [\"NaN\"] * pages\n",
    "    if year >= 2019:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] = (\n",
    "                \"https://results.virginmoneylondonmarathon.com/\"\n",
    "                + str(year)\n",
    "                + \"/?page=\"\n",
    "                + str(i + 1)\n",
    "                + \"&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=\"\n",
    "                + sex\n",
    "                + \"&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name\"\n",
    "            )\n",
    "    elif year >= 2014:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] = (\n",
    "                \"https://results.virginmoneylondonmarathon.com/\"\n",
    "                + str(year)\n",
    "                + \"/?page=\"\n",
    "                + str(i + 1)\n",
    "                + \"&event=MAS&num_results=1000&pid=list&search%5Bage_class%5D=%25&search%5Bsex%5D=\"\n",
    "                + sex\n",
    "            )\n",
    "    elif year >= 2010:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] = (\n",
    "                \"https://results.virginmoneylondonmarathon.com/\"\n",
    "                + str(year)\n",
    "                + \"/index.php?page=\"\n",
    "                + str(i + 1)\n",
    "                + \"&event=MAS&num_results=1000&pid=search&search%5Bsex%5D=\"\n",
    "                + sex\n",
    "            )\n",
    "    return urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Time to get to work\n",
    "\n",
    "Use get_virgin_urls with a list of page numbers (need to create loop for that) and a range of years to produce the list of urls that we will iterate over\n",
    "\n",
    "NOTE: Is possible to scrape 2010, but slightly diff. format so need to produce a different function for that."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get no. of pages using technique like\n",
    "# Not kept in/included in functions because requests take forever!\n",
    "# url1 = 'https://results.virginmoneylondonmarathon.com/2020/?page='\n",
    "# url2 = '&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D='\n",
    "# url3 = '&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "# site_m=requests.get(url1+'1'+url2+'M' +url3).text\n",
    "# site_w=requests.get(url1+'1'+url2+'W' +url3).text\n",
    "# soup_m = BeautifulSoup(site_m,'lxml')\n",
    "# soup_w = BeautifulSoup(site_w,'lxml')\n",
    "\n",
    "# m_pages = int(soup_m.find(class_='pages').text[-4:-2])\n",
    "# w_pages = int(soup_w.find(class_='pages').text[-4:-2])\n",
    "# print(m_pages, w_pages)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "urls = []\n",
    "# Get no. of pages using technique like\n",
    "# Not kept in/included in functions because requests take forever!\n",
    "# site_m=requests.get(url1+'1'+url2+'M').text\n",
    "# site_w=requests.get(url1+'1'+url2+'W').text\n",
    "# soup_m = BeautifulSoup(site_m,'lxml')\n",
    "# soup_w = BeautifulSoup(site_w,'lxml')\n",
    "\n",
    "# m_pages = int(soup_m.find(class_='pages').text[-4:-2])\n",
    "# w_pages = int(soup_w.find(class_='pages').text[-4:-2])\n",
    "# print(m_pages, w_pages)\n",
    "pages_men = [23, 24, 23, 23, 24, 24, 24, 24, 29, 22]\n",
    "pages_women = [13, 14, 13, 14, 15, 16, 16, 17, 21, 22]\n",
    "for i, year in enumerate(range(2011, 2021)):\n",
    "    w_urls = get_virgin_urls(\"W\", pages_women[i], year)\n",
    "    m_urls = get_virgin_urls(\"M\", pages_men[i], year)\n",
    "    urls = urls + m_urls + w_urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell uses ```multiprocess.pool``` to divide the work of making requests and parsing between a number of worker processes. This currently doesn't lead to any appreciable improvement in speed, needs further investigation! Possibly need to investigate proper threading.\n",
    "\n",
    "This process requires an iterable and a function to apply it over."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "# #Setup multiprocessing and start scraping!\n",
    "# pool = Pool(8)\n",
    "# #Scrape multiprocessing\n",
    "# data = pool.map(get_results, urls)\n",
    "# #Cleanup after yourself\n",
    "# pool.terminate()\n",
    "# pool.join()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell does a similar process, but using multithreading instead of multiprocessing, via ```concurrent.futures.ThreadPoolExecutor```."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Warning: Takes ~10mins to complete!\n",
    "# Trying using multithreading instead of multiprocessing\n",
    "MAX_THREADS = 30\n",
    "threads = min(MAX_THREADS, len(urls))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "    data = list(executor.map(get_results, urls))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get dataframe from list of df (sep cell to allow for recreation without re-parsing)\n",
    "results = pd.concat(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Some data cleaning\n",
    "# Remove leftover titles\n",
    "results[\"Club\"] = results[\"Club\"].str.replace(\"Club\", \"\", regex=False)\n",
    "results[\"Running Number\"] = results[\"Running Number\"].str.replace(\n",
    "    \"Running Number\", \"\", regex=False\n",
    ")\n",
    "results[\"Running Number\"] = results[\"Running Number\"].str.replace(\n",
    "    \"Runner Number\", \"\", regex=False\n",
    ")\n",
    "results[\"Category\"] = results[\"Category\"].str.replace(\"Category\", \"\", regex=False)\n",
    "results[\"Finish\"] = results[\"Finish\"].str.replace(\"Finish\", \"\", regex=False)\n",
    "\n",
    "# Extract country groups, like (USA), from Name group\n",
    "results[\"Country\"] = results[\"Name\"].str.extract(r\"(\\([A-Z]{3,}\\))\")\n",
    "# Remove brackets in country\n",
    "results[\"Country\"] = results[\"Country\"].str.replace(r\"\\(|\\)\", \"\", regex=True)\n",
    "# Remove country group from name column\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\([A-Z]{3}\\))\", \"\", regex=True)\n",
    "\n",
    "# Split first/lastname into new columns\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(»)\", \"\", regex=True)\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\n)\", \"\", regex=True)\n",
    "LastFirst = results[\"Name\"].str.split(pat=\",\", n=1, expand=True)\n",
    "results[\"FirstName\"], results[\"LastName\"] = LastFirst[1], LastFirst[0]\n",
    "# Remove comma from Name column, so that this can be saved as a CSV ----- Must happen after splitting Name into two cols!!\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\,)\", \"\", regex=True)\n",
    "\n",
    "# Change W to F for 2020\n",
    "results[\"Sex\"] = results[\"Sex\"].str.replace(\"W\", \"F\")\n",
    "\n",
    "# Create DSQ column for did not finish results, to avoid removing info when using nan\n",
    "results[\"DSQ\"] = results[\"Finish\"] == \"DSQ\"\n",
    "\n",
    "\n",
    "# Replace non-standard '–' with NaN for missing vals\n",
    "results = results.replace(\"DSQ\", np.nan)\n",
    "results = results.replace(\"–\", np.nan)\n",
    "results = results.replace(\"\", np.nan)\n",
    "\n",
    "# Delete odd race number row - gives fastest male/female so is duplicate\n",
    "results = results.loc[\n",
    "    (results[\"Running Number\"] != \"RM9999\") & (results[\"Running Number\"] != \"RF9999\")\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = results.astype(\n",
    "    {\n",
    "        \"Place (Overall)\": \"float64\",\n",
    "        \"Place (Gender)\": \"float64\",\n",
    "        \"Place (Category)\": \"float64\",\n",
    "        \"Name\": str,\n",
    "        \"Sex\": str,\n",
    "        \"Club\": str,\n",
    "        \"Running Number\": str,\n",
    "        \"Category\": \"category\",\n",
    "        \"Year\": \"Int64\",\n",
    "    }\n",
    ")\n",
    "# Due to an irritating bug with converting objects to Int64, needed to first convert to float and then to int\n",
    "results = results.astype(\n",
    "    {\"Place (Overall)\": \"Int64\", \"Place (Gender)\": \"Int64\", \"Place (Category)\": \"Int64\"}\n",
    ")\n",
    "results[\"Finish\"] = pd.to_timedelta(results[\"Finish\"])\n",
    "results[\"Finish (Total Seconds)\"] = results[\"Finish\"].dt.total_seconds()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's see what we've got\n",
    "results.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# And save them in a csv\n",
    "results.to_csv(\n",
    "    r\"C:\\Users\\michael.walshe\\Documents\\Python Projects\\scrape_london_marathon\\London_Marathon_Big.csv\",\n",
    "    index=False,\n",
    "    header=True,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Profiling function to find bottlenecks, need to speed up parser more???\n",
    "# url = 'https://results.virginmoneylondonmarathon.com/2019/?page=1&event=ALL&'+ \\\n",
    "#       'num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=M&search%5Bage_'+ \\\n",
    "#        'class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "# %lprun -f get_results_new get_results_new(url, \"M\", 2019)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0141990a20678273a854e9f710d7438d35c6e8d8fa786e498058914c4bbad7160",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}