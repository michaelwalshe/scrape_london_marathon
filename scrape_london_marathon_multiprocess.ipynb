{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Setting up required libraries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import requests\r\n",
    "import re\r\n",
    "import concurrent.futures  # to allow multithreading\r\n",
    "from bs4 import BeautifulSoup, SoupStrainer  # navigate through web pages\r\n",
    "from typing import Optional"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Function Definitions:\r\n",
    "Two main functions, get_results old and new. These correspond to the two website styles. Also functions to generate urls, and wrapper function to be applied with map (this enables the multiprocessing/multithreaded approach). The two get_results functions have been superceded by a single function that checks years and sets values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "def get_results_table(url: str, sex: str, year: int) -> pd.DataFrame:\r\n",
    "    \"\"\"Scrape london marathon\"\"\"\r\n",
    "\r\n",
    "    # Set parsing values for different years (different page layouts)\r\n",
    "    if year >= 2019:\r\n",
    "        strainer_expr = 'class_ = \"section-main\"'\r\n",
    "        row_expr = 'class_ = \"list-group-item\"'\r\n",
    "        cell_expr = 'class_ = \"list-field\"'\r\n",
    "    else:\r\n",
    "        strainer_expr = '\"tbody\"'\r\n",
    "        row_expr = '\"tr\"'\r\n",
    "        cell_expr = '\"td\"'\r\n",
    "    \r\n",
    "    if year ==  2021:\r\n",
    "        row_indexes = [0, 1, 2, 3, 4, 5, 6, 9]\r\n",
    "    elif year == 2014:\r\n",
    "        row_indexes = [0, 1, 2, 3, 5, 6, 7, 9]\r\n",
    "    else:\r\n",
    "        row_indexes = [0, 1, 2, 3, 4, 5, 6, 8]\r\n",
    "\r\n",
    "    site = requests.get(url).text\r\n",
    "    # Soup strainer restricts content to speed up soup\r\n",
    "    strainer = SoupStrainer(eval(strainer_expr))\r\n",
    "\r\n",
    "    soup = BeautifulSoup(site, \"lxml\", parse_only=strainer)\r\n",
    "\r\n",
    "    # Loop through each row and column to create a list of cells\r\n",
    "    my_table = []\r\n",
    "    for row in soup.find_all(eval(row_expr)):\r\n",
    "        row_data = []\r\n",
    "        for cell in row.find_all(eval(cell_expr)):\r\n",
    "            alt_text = cell.find(\"span\")\r\n",
    "            if alt_text is not None:\r\n",
    "                cell = alt_text[\"title\"]\r\n",
    "            else:\r\n",
    "                cell = cell.text\r\n",
    "            row_data.append(cell)\r\n",
    "\r\n",
    "        # If the row isn't empty, then create a dict of the row to create dataframe from\r\n",
    "        if row_data:\r\n",
    "            data_item = {\r\n",
    "                \"Place (Overall)\": row_data[row_indexes[0]],\r\n",
    "                \"Place (Gender)\": row_data[row_indexes[1]],\r\n",
    "                \"Place (Category)\": row_data[row_indexes[2]],\r\n",
    "                \"Name\": row_data[row_indexes[3]],\r\n",
    "                \"Sex\": sex,\r\n",
    "                \"Club\": row_data[row_indexes[4]],\r\n",
    "                \"Running Number\": row_data[row_indexes[5]],\r\n",
    "                \"Category\": row_data[row_indexes[6]],\r\n",
    "                \"Finish\": row_data[row_indexes[7]],\r\n",
    "                \"Year\": year,\r\n",
    "            }\r\n",
    "            my_table.append(data_item)\r\n",
    "\r\n",
    "    results = pd.DataFrame(my_table).iloc[1:]  # Strip table header\r\n",
    "\r\n",
    "    return results\r\n",
    "\r\n",
    "\r\n",
    "def get_results_new(url: str, sex: str, year :int) -> pd.DataFrame:\r\n",
    "    \"\"\"Function to scrape modern virgin london marathon results page (2019 to 2021)\"\"\"\r\n",
    "\r\n",
    "    site = requests.get(url).text\r\n",
    "    # Soup strainer restricts content to speed up soup\r\n",
    "    strainer = SoupStrainer(class_=\"section-main\")\r\n",
    "    soup = BeautifulSoup(site, \"lxml\", parse_only=strainer)\r\n",
    "    # fields = soup.find(class_='section-main')\r\n",
    "\r\n",
    "    # Loop through each row and column to create a list of cells\r\n",
    "    my_table = []\r\n",
    "    for row in soup.find_all(class_=\"list-group-item\"):\r\n",
    "        row_data = []\r\n",
    "        for cell in row.find_all(class_=\"list-field\"):\r\n",
    "            row_data.append(cell.text)\r\n",
    "\r\n",
    "        # If the row isn't empty, then create a dict of the row to create dataframe from\r\n",
    "        if row_data:\r\n",
    "            data_item = {\r\n",
    "                \"Place (Overall)\": row_data[0],\r\n",
    "                \"Place (Gender)\": row_data[1],\r\n",
    "                \"Place (Category)\": row_data[2],\r\n",
    "                \"Name\": row_data[3],\r\n",
    "                \"Sex\": sex,\r\n",
    "                \"Club\": row_data[4],\r\n",
    "                \"Running Number\": row_data[5],\r\n",
    "                \"Category\": row_data[6],\r\n",
    "                \"Finish\": row_data[8],\r\n",
    "                \"Year\": year,\r\n",
    "            }\r\n",
    "            my_table.append(data_item)\r\n",
    "\r\n",
    "    results = pd.DataFrame(my_table).iloc[1:]  # Strip table header\r\n",
    "\r\n",
    "    return results\r\n",
    "\r\n",
    "\r\n",
    "def get_results_old(url: str, sex: str, year: int) -> pd.DataFrame:\r\n",
    "    \"\"\"Function to scrape old virgin london marathon results page (2014 to 2018)\"\"\"\r\n",
    "\r\n",
    "    site = requests.get(url).text  # Use requests to get content from site\r\n",
    "    strainer = SoupStrainer(\"tbody\")  # Soup strainer restricts content to sped up soup\r\n",
    "    soup = BeautifulSoup(site, \"lxml\", parse_only=strainer)  # Parse the html\r\n",
    "\r\n",
    "    my_table = []\r\n",
    "    for row in soup.find_all(\"tr\"):\r\n",
    "        row_data = []\r\n",
    "        for cell in row.find_all(\"td\"):\r\n",
    "            # Check if cell has alt text, if so use that as data\r\n",
    "            alt_text = cell.find(\"span\")\r\n",
    "            if alt_text is not None:\r\n",
    "                cell = alt_text[\"title\"]\r\n",
    "            else:\r\n",
    "                cell = cell.text\r\n",
    "            row_data.append(cell)\r\n",
    "\r\n",
    "        # If the row isn't empty, then create a dict of the row to create dataframe from\r\n",
    "        if len(row_data) > 0 and year != 2014:\r\n",
    "            data_item = {\r\n",
    "                \"Place (Overall)\": row_data[0],\r\n",
    "                \"Place (Gender)\": row_data[1],\r\n",
    "                \"Place (Category)\": row_data[2],\r\n",
    "                \"Name\": row_data[3],\r\n",
    "                \"Sex\": sex,\r\n",
    "                \"Club\": row_data[4],\r\n",
    "                \"Running Number\": row_data[5],\r\n",
    "                \"Category\": row_data[6],\r\n",
    "                \"Finish\": row_data[8],\r\n",
    "                \"Year\": year,\r\n",
    "            }\r\n",
    "            my_table.append(data_item)\r\n",
    "        elif len(row_data) > 0 and year == 2014:\r\n",
    "            data_item = {\r\n",
    "                \"Place (Overall)\": row_data[0],\r\n",
    "                \"Place (Gender)\": row_data[1],\r\n",
    "                \"Place (Category)\": row_data[2],\r\n",
    "                \"Name\": row_data[3],\r\n",
    "                \"Sex\": sex,\r\n",
    "                \"Club\": row_data[5],\r\n",
    "                \"Running Number\": row_data[6],\r\n",
    "                \"Category\": row_data[7],\r\n",
    "                \"Finish\": row_data[9],\r\n",
    "                \"Year\": year,\r\n",
    "            }\r\n",
    "            my_table.append(data_item)\r\n",
    "\r\n",
    "    results = pd.DataFrame(my_table)  # Strip table header\r\n",
    "\r\n",
    "    return results\r\n",
    "\r\n",
    "\r\n",
    "def get_results(url):\r\n",
    "    \"\"\"Function chooses what results func to apply. Used to allow single function for pool.map\"\"\"\r\n",
    "\r\n",
    "    # Get year and sex from the URL\r\n",
    "    year = int(re.search(r\"\\.com/(\\d{4})/\", url).group(1))\r\n",
    "    sex = re.search(r\"sex%5D=(\\w)\", url).group(1)\r\n",
    "    page = re.search(r\"page=(.*?)&event=\", url).group(1)\r\n",
    "    print(f\"Getting results for {sex} in {year}, page {page}\")\r\n",
    "    # if year >= 2019:\r\n",
    "    #     data = get_results_new(url, sex, year)\r\n",
    "    # elif year >= 2010:\r\n",
    "    #     data = get_results_old(url, sex, year)\r\n",
    "    # else:\r\n",
    "    #     data = None\r\n",
    "\r\n",
    "    data = get_results_table(url, sex, year)\r\n",
    "\r\n",
    "    print(f\"Finished getting results for {sex} in {year}, page {page}\")\r\n",
    "    return data\r\n",
    "\r\n",
    "\r\n",
    "def generate_virgin_urls(sex, pages, year):\r\n",
    "    \"\"\"Get a list of urls, this is needed to be used to apply function to to then use multiprocessing\"\"\"\r\n",
    "\r\n",
    "    urls = [\"NaN\"] * pages\r\n",
    "    if year >= 2019:\r\n",
    "        for i in range(pages):\r\n",
    "            urls[i] = (\r\n",
    "                f\"https://results.virginmoneylondonmarathon.com/\"\r\n",
    "                + str(year)\r\n",
    "                + \"/?page=\"\r\n",
    "                + str(i + 1)\r\n",
    "                + \"&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=\"\r\n",
    "                + sex\r\n",
    "                + \"&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name\"\r\n",
    "            )\r\n",
    "\r\n",
    "    elif year >= 2014:\r\n",
    "        for i in range(pages):\r\n",
    "            urls[i] = (\r\n",
    "                \"https://results.virginmoneylondonmarathon.com/\"\r\n",
    "                + str(year)\r\n",
    "                + \"/?page=\"\r\n",
    "                + str(i + 1)\r\n",
    "                + \"&event=MAS&num_results=1000&pid=list&search%5Bage_class%5D=%25&search%5Bsex%5D=\"\r\n",
    "                + sex\r\n",
    "            )\r\n",
    "\r\n",
    "    elif year >= 2010:\r\n",
    "        for i in range(pages):\r\n",
    "            urls[i] = (\r\n",
    "                \"https://results.virginmoneylondonmarathon.com/\"\r\n",
    "                + str(year)\r\n",
    "                + \"/index.php?page=\"\r\n",
    "                + str(i + 1)\r\n",
    "                + \"&event=MAS&num_results=1000&pid=search&search%5Bsex%5D=\"\r\n",
    "                + sex\r\n",
    "            )\r\n",
    "\r\n",
    "    return urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Time to get to work\n",
    "\n",
    "Use get_virgin_urls with a list of page numbers (need to create loop for that) and a range of years to produce the list of urls that we will iterate over\n",
    "\n",
    "NOTE: Is possible to scrape 2010, but slightly diff. format so need to produce a different function for that."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get no. of pages using technique like\r\n",
    "# Not kept in/included in functions because requests take forever!\r\n",
    "# url1 = 'https://results.virginmoneylondonmarathon.com/2020/?page='\r\n",
    "# url2 = '&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D='\r\n",
    "# url3 = '&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\r\n",
    "\r\n",
    "# site_m=requests.get(url1+'1'+url2+'M' +url3).text\r\n",
    "# site_w=requests.get(url1+'1'+url2+'W' +url3).text\r\n",
    "# soup_m = BeautifulSoup(site_m,'lxml')\r\n",
    "# soup_w = BeautifulSoup(site_w,'lxml')\r\n",
    "\r\n",
    "# m_pages = int(soup_m.find(class_='pages').text[-4:-2])\r\n",
    "# w_pages = int(soup_w.find(class_='pages').text[-4:-2])\r\n",
    "# print(m_pages, w_pages)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\r\n",
    "urls = []\r\n",
    "years = None # Set to list of years if just want some\r\n",
    "\r\n",
    "pages_men = {\r\n",
    "    2011: 23,\r\n",
    "    2012: 24,\r\n",
    "    2013: 23,\r\n",
    "    2014: 23,\r\n",
    "    2015: 24,\r\n",
    "    2016: 24,\r\n",
    "    2017: 24,\r\n",
    "    2018: 24,\r\n",
    "    2019: 29,\r\n",
    "    2020: 22,\r\n",
    "    2021: 25,\r\n",
    "}\r\n",
    "pages_women = {\r\n",
    "    2011: 13,\r\n",
    "    2012: 14,\r\n",
    "    2013: 13,\r\n",
    "    2014: 14,\r\n",
    "    2015: 15,\r\n",
    "    2016: 16,\r\n",
    "    2017: 16,\r\n",
    "    2018: 17,\r\n",
    "    2019: 21,\r\n",
    "    2020: 22,\r\n",
    "    2021: 17,\r\n",
    "}\r\n",
    "\r\n",
    "print(\"Generating URLS...\")\r\n",
    "# Too lazy to setup dataframe for years/pages/gender, need to\r\n",
    "# Check if years to search has been set\r\n",
    "if years is None:\r\n",
    "    years = [yr for yr in pages_men.keys() if yr != 2020] # 2020 has disappeared?\r\n",
    "\r\n",
    "for year in years:\r\n",
    "    w_urls = generate_virgin_urls(\"W\", pages_women[year], year)\r\n",
    "    m_urls = generate_virgin_urls(\"M\", pages_men[year], year)\r\n",
    "    urls = urls + m_urls + w_urls"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell uses ```multiprocess.pool``` to divide the work of making requests and parsing between a number of worker processes. This currently doesn't lead to any appreciable improvement in speed, needs further investigation! Possibly need to investigate proper threading.\n",
    "\n",
    "This process requires an iterable and a function to apply it over."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %%timeit -n 1 -r 1\r\n",
    "# #Setup multiprocessing and start scraping!\r\n",
    "# pool = Pool(8)\r\n",
    "# #Scrape multiprocessing\r\n",
    "# data = pool.map(get_results, urls)\r\n",
    "# #Cleanup after yourself\r\n",
    "# pool.terminate()\r\n",
    "# pool.join()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell does a similar process, but using multithreading instead of multiprocessing, via ```concurrent.futures.ThreadPoolExecutor```."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Warning: Takes ~10mins to complete!\r\n",
    "# Trying using multithreading instead of multiprocessing\r\n",
    "MAX_THREADS = 30\r\n",
    "threads = min(MAX_THREADS, len(urls))\r\n",
    "\r\n",
    "print(\"Beginning data extract....\")\r\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\r\n",
    "    data = list(executor.map(get_results, urls))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get dataframe from list of df (sep cell to allow for recreation without re-parsing)\r\n",
    "results = pd.concat(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Some data cleaning\r\n",
    "# Remove leftover titles\r\n",
    "results[\"Club\"] = results[\"Club\"].str.replace(\"Club\", \"\", regex=False)\r\n",
    "results[\"Running Number\"] = results[\"Running Number\"].str.replace(\r\n",
    "    \"Running Number\", \"\", regex=False\r\n",
    ")\r\n",
    "results[\"Running Number\"] = results[\"Running Number\"].str.replace(\r\n",
    "    \"Runner Number\", \"\", regex=False\r\n",
    ")\r\n",
    "results[\"Category\"] = results[\"Category\"].str.replace(\"Category\", \"\", regex=False)\r\n",
    "results[\"Finish\"] = results[\"Finish\"].str.replace(\"Finish\", \"\", regex=False)\r\n",
    "\r\n",
    "# Extract country groups, like (USA), from Name group\r\n",
    "results[\"Country\"] = results[\"Name\"].str.extract(r\"(\\([A-Z]{3,}\\))\")\r\n",
    "# Remove brackets in country\r\n",
    "results[\"Country\"] = results[\"Country\"].str.replace(r\"\\(|\\)\", \"\", regex=True)\r\n",
    "# Remove country group from name column\r\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\([A-Z]{3}\\))\", \"\", regex=True)\r\n",
    "\r\n",
    "# Split first/lastname into new columns\r\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(»)\", \"\", regex=True)\r\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\n)\", \"\", regex=True)\r\n",
    "last_first = results[\"Name\"].str.split(pat=\",\", n=1, expand=True)\r\n",
    "results[\"FirstName\"], results[\"LastName\"] = last_first[1], last_first[0]\r\n",
    "# Remove comma from Name column, so that this can be saved as a CSV ----- Must happen after splitting Name into two cols!!\r\n",
    "results[\"Name\"] = results[\"Name\"].str.replace(r\"(\\,)\", \"\", regex=True)\r\n",
    "\r\n",
    "# Change W to F for 2020\r\n",
    "results[\"Sex\"] = results[\"Sex\"].str.replace(\"W\", \"F\")\r\n",
    "\r\n",
    "# Create DSQ column for did not finish results, to avoid removing info when using nan\r\n",
    "results[\"DSQ\"] = results[\"Finish\"] == \"DSQ\"\r\n",
    "\r\n",
    "# Replace non-standard '–' with NaN for missing vals\r\n",
    "results = results.replace(\"DSQ\", np.nan)\r\n",
    "results = results.replace(\"–\", np.nan)\r\n",
    "results = results.replace(\"\", np.nan)\r\n",
    "\r\n",
    "# Delete odd race number row - gives fastest male/female so is duplicate\r\n",
    "results = results.loc[\r\n",
    "    (results[\"Running Number\"] != \"RM9999\")\r\n",
    "    & (results[\"Running Number\"] != \"RF9999\")\r\n",
    "]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "results = results.astype(\r\n",
    "    {\r\n",
    "        \"Place (Overall)\": \"float64\",\r\n",
    "        \"Place (Gender)\": \"float64\",\r\n",
    "        \"Place (Category)\": \"float64\",\r\n",
    "        \"Name\": str,\r\n",
    "        \"Sex\": str,\r\n",
    "        \"Club\": str,\r\n",
    "        \"Running Number\": str,\r\n",
    "        \"Category\": \"category\",\r\n",
    "        \"Year\": \"Int64\",\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "# Due to an irritating bug with converting objects to Int64, needed to first convert to float and then to int\r\n",
    "results = results.astype(\r\n",
    "    {\r\n",
    "        \"Place (Overall)\": \"Int64\",\r\n",
    "        \"Place (Gender)\": \"Int64\",\r\n",
    "        \"Place (Category)\": \"Int64\",\r\n",
    "    }\r\n",
    ")\r\n",
    "results[\"Finish\"] = pd.to_timedelta(results[\"Finish\"])\r\n",
    "results[\"Finish (Total Seconds)\"] = results[\"Finish\"].dt.total_seconds()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's see what we've got\r\n",
    "results.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# And save them in a csv\r\n",
    "results.to_csv(\r\n",
    "    r\"C:\\Users\\michael.walshe\\Documents\\Python Projects\\scrape_london_marathon\\London_Marathon_Big.csv\",\r\n",
    "    index=False,\r\n",
    "    header=True,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Profiling function to find bottlenecks, need to speed up parser more???\r\n",
    "# url = 'https://results.virginmoneylondonmarathon.com/2019/?page=1&event=ALL&'+ \\\r\n",
    "#       'num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=M&search%5Bage_'+ \\\r\n",
    "#        'class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\r\n",
    "\r\n",
    "# %lprun -f get_results_new get_results_new(url, \"M\", 2019)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('anc': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "interpreter": {
   "hash": "c8b21321f26bdc6ac60d53849d9315e38be639ed399267d59045732bea44f11b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}