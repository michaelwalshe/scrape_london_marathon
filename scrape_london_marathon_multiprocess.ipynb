{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import lxml\n",
    "import cchardet\n",
    "import re\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from multiprocessing.dummy import Pool  # This is a thread-based Pool\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "source": [
    "# Function Definitions:\n",
    "Two main functions, get_results old and new. These correspond to the two website styles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_new(url, sex, year):\n",
    "    #Function to scrape modern virgin london marathon results page (2020 and 2019)\n",
    "    #Set up empty dataframe for results\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    #Use requests to get content from site\n",
    "    site=requests.get(url).content\n",
    "    #Soup strainer restricts content to sped up soup\n",
    "    strainer = SoupStrainer(class_=\"section-main\")\n",
    "    #Parse the html\n",
    "    soup = BeautifulSoup(site,'lxml', parse_only=strainer)\n",
    "    #fields = soup.find(class_='section-main')\n",
    "\n",
    "    #Loop through each row and column to create a list of cells\n",
    "    my_table = []\n",
    "    for row in soup.find_all(class_='list-group-item'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(class_='list-field'):\n",
    "            row_data.append(cell.text)\n",
    "        \n",
    "        #If the row isn't empty, then create a dict of the row to create datafram from\n",
    "        if(len(row_data) > 0):\n",
    "            data_item = {\"Place (Overall)\": row_data[0],\n",
    "                        \"Place (Gender)\": row_data[1],\n",
    "                        \"Place (Category)\": row_data[2],\n",
    "                        \"Name\": row_data[3],\n",
    "                        \"Sex\": sex,\n",
    "                        \"Club\": row_data[4],\n",
    "                        \"Running Number\": row_data[5],\n",
    "                        \"Category\": row_data[6],\n",
    "                        \"Finish\": row_data[7],\n",
    "                        \"Year\": year\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "        \n",
    "    #Strip table header\n",
    "    df = pd.DataFrame(my_table).iloc[1:]\n",
    "        \n",
    "    #Append to results\n",
    "    results = results.append(df)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_old(url, sex, year):\n",
    "    #Function to scrape old virgin london marathon results page (2014 to 2018)\n",
    "    #Set up empty dataframe for results\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    #Use requests to get content from site\n",
    "    site=requests.get(url).content\n",
    "    #Soup strainer restricts content to sped up soup\n",
    "    strainer = SoupStrainer('tbody')\n",
    "    #Parse the html\n",
    "    soup = BeautifulSoup(site,'lxml', parse_only=strainer)\n",
    "\n",
    "    my_table = []\n",
    "    for row in soup.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            #Check if cell has alt text, if so use that as data\n",
    "            alt_text = cell.find('span')\n",
    "            if alt_text != None:\n",
    "                cell = alt_text['title']\n",
    "            else:\n",
    "                cell = cell.text\n",
    "            row_data.append(cell)\n",
    "            \n",
    "        #If the row isn't empty, then create a dict of the row to create datafram from\n",
    "        if(len(row_data) > 0):\n",
    "            data_item = {\"Place (Overall)\": row_data[0],\n",
    "                            \"Place (Gender)\": row_data[1],\n",
    "                            \"Place (Category)\": row_data[2],\n",
    "                            \"Name\": row_data[3],\n",
    "                            \"Sex\": sex,\n",
    "                            \"Club\": row_data[4],\n",
    "                            \"Running Number\": row_data[5],\n",
    "                            \"Category\": row_data[6],\n",
    "                            \"Finish\": row_data[8],\n",
    "                            \"Year\": year\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "\n",
    "    #Strip table header\n",
    "    df = pd.DataFrame(my_table).iloc[1:]\n",
    "\n",
    "    #Append to results\n",
    "    results = results.append(df)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(url):\n",
    "    #Function choose what results func to apply\n",
    "\n",
    "    #Check what year the url is\n",
    "    year = int(re.search('\\.com\\/(\\d{4})\\/', url).group(1))\n",
    "    sex = re.search('sex%5D=(\\w)', url).group(1)\n",
    "    if year >= 2019:\n",
    "        data = get_results_new(url, sex, year)\n",
    "    elif year >= 2010:\n",
    "        data = get_results_old(url, sex, year)\n",
    "    else:\n",
    "        data = None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_virgin_urls(sex, pages, year):\n",
    "    # Get a list of urls, this is needed to be used to apply function to to then use multiprocessing\n",
    "    urls = ['NaN'] * pages\n",
    "    if year >= 2019:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] =  'https://results.virginmoneylondonmarathon.com/' \\\n",
    "                        +str(year) \\\n",
    "                        +'/?page=' \\\n",
    "                        +str(i+1) \\\n",
    "                        +'&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=' \\\n",
    "                        +sex \\\n",
    "                        +'&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "\n",
    "    elif year >= 2014:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] = ('https://results.virginmoneylondonmarathon.com/'\n",
    "                        +str(year)\n",
    "                        +'/?page='\n",
    "                        +str(i+1)\n",
    "                        +'&event=MAS&num_results=1000&pid=list&search%5Bage_class%5D=%25&search%5Bsex%5D='\n",
    "                        +sex)\n",
    "\n",
    "    elif year >= 2010:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] = ('https://results.virginmoneylondonmarathon.com/'\n",
    "                        +str(year)\n",
    "                        +'/index.php?page='\n",
    "                        +str(i+1)\n",
    "                        +'&event=MAS&num_results=1000&pid=search&search%5Bsex%5D='\n",
    "                        +sex)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "#Get no. of pages using technique like\n",
    "#Not kept in/included in functions because requests take forever!\n",
    "# site_m=requests.get(url1+'1'+url2+'M').text\n",
    "# site_w=requests.get(url1+'1'+url2+'W').text\n",
    "# soup_m = BeautifulSoup(site_m,'lxml')\n",
    "# soup_w = BeautifulSoup(site_w,'lxml')\n",
    "\n",
    "# m_pages = int(soup_m.find(class_='pages').text[-4:-2])\n",
    "# w_pages = int(soup_w.find(class_='pages').text[-4:-2])\n",
    "# print(m_pages, w_pages)\n",
    "pages_men = [23, 24, 23, 25, 23, 24, 24, 24, 24, 25, 22]\n",
    "pages_women = [13, 14, 13, 13, 14, 15, 16, 16, 17, 18, 22]\n",
    "for i, year in enumerate(range(2010, 2020)):\n",
    "    w_urls = get_virgin_urls('W', pages_women[i], year)\n",
    "    m_urls = get_virgin_urls('M', pages_men[i], year)\n",
    "    new_urls = m_urls + w_urls\n",
    "    urls = urls + new_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shorter urls to test with\n",
    "urls = []\n",
    "pages_men = [22, 25]\n",
    "pages_women = [22, 18]\n",
    "for i, year in enumerate(range(2019, 2020)):\n",
    "    w_urls = get_virgin_urls('W', pages_women[i], year)\n",
    "    m_urls = get_virgin_urls('M', pages_men[i], year)\n",
    "    new_urls = m_urls + w_urls\n",
    "    urls = urls + new_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame()\n",
    "#Setup multiprocessing and start scraping!\n",
    "pool = Pool(8)\n",
    "#Scrape multiprocessing\n",
    "results2 = pool.map(get_results, urls)\n",
    "df2 = pd.concat(results2)\n",
    "#Cleanup after yourself\n",
    "pool.terminate()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some quick data cleaning\n",
    "london_marathon_results['Club'] = london_marathon_results['Club'].str.replace(\"Club\", \"\", regex=False)\n",
    "london_marathon_results['Running Number'] = london_marathon_results['Running Number'].str.replace(\"Running Number\", \"\", regex=False)\n",
    "london_marathon_results['Category'] = london_marathon_results['Category'].str.replace(\"Category\", \"\", regex=False)\n",
    "london_marathon_results['Finish'] = london_marathon_results['Finish'].str.replace(\"Finish\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we've got\n",
    "london_marathon_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And quickly save them in a csv\n",
    "london_marathon_results.to_csv(r'C:\\Users\\michael.walshe\\Documents\\Python and CAS\\London_Marathon.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Profiling function to find bottlenecks, need to speed up parser more???\n",
    "# url = 'https://results.virginmoneylondonmarathon.com/2019/?page=1&event=ALL&'+ \\\n",
    "#       'num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=M&search%5Bage_'+ \\\n",
    "#        'class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "# %lprun -f get_results_new get_results_new(url, \"M\", 2019)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd00188e1d2bc53a9294aae56bf1ea31698e906b60a882ff91489addc94996f1fb7",
   "display_name": "Python 3.8.5 64-bit ('testenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}