{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import lxml # alternative xml parser for bs4\n",
    "import cchardet # c processing of characters, faster?\n",
    "import re # regex\n",
    "import concurrent.futures # to allow multithreading\n",
    "from bs4 import BeautifulSoup, SoupStrainer # navigate through web pages\n",
    "from multiprocessing import Pool, cpu_count # to allow multiprocessing"
   ]
  },
  {
   "source": [
    "# Function Definitions:\n",
    "Two main functions, get_results old and new. These correspond to the two website styles. Also functions to generate urls, and wrapper function to be applied with map (this enables the multiprocessing/multithreaded approach)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_new(url, sex, year):\n",
    "    #Function to scrape modern virgin london marathon results page (2020 and 2019)\n",
    "    \n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    site=requests.get(url).text                                 #Use requests to get content from site\n",
    "    strainer = SoupStrainer(class_=\"section-main\")              #Soup strainer restricts content to sped up soup\n",
    "    soup = BeautifulSoup(site,'lxml', parse_only=strainer)      #Parse the html\n",
    "    #fields = soup.find(class_='section-main')\n",
    "\n",
    "    #Loop through each row and column to create a list of cells\n",
    "    my_table = []\n",
    "    for row in soup.find_all(class_='list-group-item'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(class_='list-field'):\n",
    "            row_data.append(cell.text)\n",
    "        \n",
    "        #If the row isn't empty, then create a dict of the row to create datafram from\n",
    "        if(len(row_data) > 0):\n",
    "            data_item = {\"Place (Overall)\": row_data[0],\n",
    "                        \"Place (Gender)\": row_data[1],\n",
    "                        \"Place (Category)\": row_data[2],\n",
    "                        \"Name\": row_data[3],\n",
    "                        \"Sex\": sex,\n",
    "                        \"Club\": row_data[4],\n",
    "                        \"Running Number\": row_data[5],\n",
    "                        \"Category\": row_data[6],\n",
    "                        \"Finish\": row_data[8],\n",
    "                        \"Year\": year\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "    \n",
    "    df = pd.DataFrame(my_table).iloc[1:]        #Strip table header\n",
    "    results = results.append(df)                #Append to results\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_old(url, sex, year):\n",
    "    #Function to scrape old virgin london marathon results page (2014 to 2018)\n",
    "    \n",
    "    results = pd.DataFrame()    #Set up empty dataframe for results\n",
    "    \n",
    "    site=requests.get(url).text                             #Use requests to get content from site\n",
    "    strainer = SoupStrainer('tbody')                        #Soup strainer restricts content to sped up soup\n",
    "    soup = BeautifulSoup(site,'lxml', parse_only=strainer)  #Parse the html\n",
    "\n",
    "    my_table = []\n",
    "    for row in soup.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all('td'):\n",
    "            #Check if cell has alt text, if so use that as data\n",
    "            alt_text = cell.find('span')\n",
    "            if alt_text != None:\n",
    "                cell = alt_text['title']\n",
    "            else:\n",
    "                cell = cell.text\n",
    "            row_data.append(cell)\n",
    "            \n",
    "        #If the row isn't empty, then create a dict of the row to create datafram from\n",
    "        if(len(row_data) > 0 and year != 2014):\n",
    "            data_item = {\"Place (Overall)\": row_data[0],\n",
    "                            \"Place (Gender)\": row_data[1],\n",
    "                            \"Place (Category)\": row_data[2],\n",
    "                            \"Name\": row_data[3],\n",
    "                            \"Sex\": sex,\n",
    "                            \"Club\": row_data[4],\n",
    "                            \"Running Number\": row_data[5],\n",
    "                            \"Category\": row_data[6],\n",
    "                            \"Finish\": row_data[8],\n",
    "                            \"Year\": year\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "        elif(len(row_data) > 0 and year == 2014):\n",
    "            data_item = {\"Place (Overall)\": row_data[0],\n",
    "                            \"Place (Gender)\": row_data[1],\n",
    "                            \"Place (Category)\": row_data[2],\n",
    "                            \"Name\": row_data[3],\n",
    "                            \"Sex\": sex,\n",
    "                            \"Club\": row_data[5],\n",
    "                            \"Running Number\": row_data[6],\n",
    "                            \"Category\": row_data[7],\n",
    "                            \"Finish\": row_data[9],\n",
    "                            \"Year\": year\n",
    "            }\n",
    "            my_table.append(data_item)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(my_table)     #Strip table header\n",
    "    results = results.append(df)    #Append to results\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(url):\n",
    "    #Function choose what results func to apply\n",
    "    #Used to allow single function for pool.map\n",
    "    year = int(re.search('\\.com\\/(\\d{4})\\/', url).group(1)) #Check what year the url is\n",
    "    sex = re.search('sex%5D=(\\w)', url).group(1)\n",
    "    if year >= 2019:\n",
    "        data = get_results_new(url, sex, year)\n",
    "    elif year >= 2010:\n",
    "        data = get_results_old(url, sex, year)\n",
    "    else:\n",
    "        data = None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_virgin_urls(sex, pages, year):\n",
    "    # Get a list of urls, this is needed to be used to apply function to to then use multiprocessing\n",
    "    urls = ['NaN'] * pages\n",
    "    if year >= 2019:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] =  'https://results.virginmoneylondonmarathon.com/' \\\n",
    "                        +str(year) \\\n",
    "                        +'/?page=' \\\n",
    "                        +str(i+1) \\\n",
    "                        +'&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=' \\\n",
    "                        +sex \\\n",
    "                        +'&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "\n",
    "    elif year >= 2014:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] = ('https://results.virginmoneylondonmarathon.com/'\n",
    "                        +str(year)\n",
    "                        +'/?page='\n",
    "                        +str(i+1)\n",
    "                        +'&event=MAS&num_results=1000&pid=list&search%5Bage_class%5D=%25&search%5Bsex%5D='\n",
    "                        +sex)\n",
    "\n",
    "    elif year >= 2010:\n",
    "        for i in range(len(urls)):\n",
    "            urls[i] = ('https://results.virginmoneylondonmarathon.com/'\n",
    "                        +str(year)\n",
    "                        +'/index.php?page='\n",
    "                        +str(i+1)\n",
    "                        +'&event=MAS&num_results=1000&pid=search&search%5Bsex%5D='\n",
    "                        +sex)\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "source": [
    "# Time to get to work\n",
    "\n",
    "Use get_virgin_urls with a list of page numbers (need to create loop for that) and a range of years to produce the list of urls that we will iterate over\n",
    "\n",
    "NOTE: Is possible to scrape 2010, but slightly diff. format so need to produce a different function for that."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get no. of pages using technique like\n",
    "#Not kept in/included in functions because requests take forever!\n",
    "# url1 = 'https://results.virginmoneylondonmarathon.com/2020/?page='\n",
    "# url2 = '&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D='\n",
    "# url3 = '&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "# site_m=requests.get(url1+'1'+url2+'M' +url3).text\n",
    "# site_w=requests.get(url1+'1'+url2+'W' +url3).text\n",
    "# soup_m = BeautifulSoup(site_m,'lxml')\n",
    "# soup_w = BeautifulSoup(site_w,'lxml')\n",
    "\n",
    "# m_pages = int(soup_m.find(class_='pages').text[-4:-2])\n",
    "# w_pages = int(soup_w.find(class_='pages').text[-4:-2])\n",
    "# print(m_pages, w_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "urls = []\n",
    "#Get no. of pages using technique like\n",
    "#Not kept in/included in functions because requests take forever!\n",
    "# site_m=requests.get(url1+'1'+url2+'M').text\n",
    "# site_w=requests.get(url1+'1'+url2+'W').text\n",
    "# soup_m = BeautifulSoup(site_m,'lxml')\n",
    "# soup_w = BeautifulSoup(site_w,'lxml')\n",
    "\n",
    "# m_pages = int(soup_m.find(class_='pages').text[-4:-2])\n",
    "# w_pages = int(soup_w.find(class_='pages').text[-4:-2])\n",
    "# print(m_pages, w_pages)\n",
    "pages_men = [23, 24, 23, 23, 24, 24, 24, 24, 29, 22]\n",
    "pages_women = [13, 14, 13, 14, 15, 16, 16, 17, 21, 22]\n",
    "for i, year in enumerate(range(2011, 2021)):\n",
    "    w_urls = get_virgin_urls('W', pages_women[i], year)\n",
    "    m_urls = get_virgin_urls('M', pages_men[i], year)\n",
    "    urls = urls + m_urls + w_urls"
   ]
  },
  {
   "source": [
    "The following cell uses ```multiprocess.pool``` to divide the work of making requests and parsing between a number of worker processes. This currently doesn't lead to any appreciable improvement in speed, needs further investigation! Possibly need to investigate proper threading.\n",
    "\n",
    "This process requires an iterable and a function to apply it over."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 1 -r 1\n",
    "# #Setup multiprocessing and start scraping!\n",
    "# pool = Pool(8)\n",
    "# #Scrape multiprocessing\n",
    "# data = pool.map(get_results, urls)\n",
    "# #Cleanup after yourself\n",
    "# pool.terminate()\n",
    "# pool.join()"
   ]
  },
  {
   "source": [
    "The following cell does a similar process, but using multithreading instead of multiprocessing, via ```concurrent.futures.ThreadPoolExecutor```."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Warning: Takes ~10mins to complete!\n",
    "#Trying using multithreading instead of multiprocessing\n",
    "MAX_THREADS = 30\n",
    "threads = min(MAX_THREADS, len(urls))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "    data = list(executor.map(get_results, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Get dataframe from list of df (sep cell to allow for recreation without re-parsing)\n",
    "results = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Some data cleaning\n",
    "#Remove leftover titles\n",
    "results['Club'] = results['Club'].str.replace(\"Club\", \"\", regex=False)\n",
    "results['Running Number'] = results['Running Number'].str.replace(\"Running Number\", \"\", regex=False)\n",
    "results['Running Number'] = results['Running Number'].str.replace(\"Runner Number\", \"\", regex=False)\n",
    "results['Category'] = results['Category'].str.replace(\"Category\", \"\", regex=False)\n",
    "results['Finish'] = results['Finish'].str.replace(\"Finish\", \"\", regex=False)\n",
    "\n",
    "#Extract country groups, like (USA), from Name group\n",
    "results['Country'] = results['Name'].str.extract(r'(\\([A-Z]{3,}\\))')\n",
    "#Remove brackets in country\n",
    "results['Country'] = results['Country'].str.replace(r'\\(|\\)', \"\")\n",
    "#Remove country group from name column\n",
    "results['Name'] = results['Name'].str.replace(r'(\\([A-Z]{3}\\))', \"\")\n",
    "\n",
    "#Split first/lastname into new columns\n",
    "results['Name'] = results['Name'].str.replace(r'(»)', \"\")\n",
    "results['Name'] = results['Name'].str.replace(r'(\\n)', \"\")\n",
    "LastFirst = results['Name'].str.split(pat=\",\", n=1, expand=True) \n",
    "results['FirstName'], results['LastName'] = LastFirst[1], LastFirst[0]\n",
    "#Remove comma from Name column, so that this can be saved as a CSV ----- Must happen after splitting Name into two cols!!\n",
    "results['Name'] = results['Name'].str.replace(r'(\\,)', \"\")\n",
    "\n",
    "#Create DSQ column for did not finish results, to avoid removing info when using nan\n",
    "results['DSQ'] = results['Finish'] == 'DSQ'\n",
    "\n",
    "\n",
    "#Replace non-standard '–' with NaN for missing vals\n",
    "results = results.replace('DSQ', np.nan)\n",
    "results = results.replace('–', np.nan)\n",
    "results = results.replace('', np.nan)\n",
    "\n",
    "#Delete odd race number row - table description not actual data\n",
    "results = results.loc[(results['Running Number'] != 'RM9999') & (results['Running Number'] != 'RF9999')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results =  results.astype({\"Place (Overall)\": 'float64',\n",
    "                           \"Place (Gender)\": 'float64',\n",
    "                           \"Place (Category)\": 'float64',\n",
    "                           \"Name\": str,\n",
    "                           \"Sex\": str,\n",
    "                           \"Club\": str,\n",
    "                           \"Running Number\": 'float64',\n",
    "                           \"Category\": 'category',\n",
    "                           \"Year\": 'Int64'})\n",
    "# Due to an irritating bug with converting objects to Int64, needed to first convert to float and then to int\n",
    "results =  results.astype({\"Place (Overall)\": 'Int64',\n",
    "                           \"Place (Gender)\": 'Int64',\n",
    "                           \"Place (Category)\": 'Int64',\n",
    "                           \"Running Number\": 'Int64'})\n",
    "results['Finish'] = pd.to_timedelta(results['Finish'])\n",
    "results['Finish (Total Seconds)'] = results['Finish'].dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's see what we've got\n",
    "results.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# And save them in a csv\n",
    "results.to_csv(r'C:\\Users\\michael.walshe\\Documents\\Python Projects\\scrape_london_marathon\\London_Marathon_Big.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Profiling function to find bottlenecks, need to speed up parser more???\n",
    "# url = 'https://results.virginmoneylondonmarathon.com/2019/?page=1&event=ALL&'+ \\\n",
    "#       'num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D=M&search%5Bage_'+ \\\n",
    "#        'class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "\n",
    "# %lprun -f get_results_new get_results_new(url, \"M\", 2019)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd00188e1d2bc53a9294aae56bf1ea31698e906b60a882ff91489addc94996f1fb7",
   "display_name": "Python 3.8.5 64-bit ('testenv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}