{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup, SoupStrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_new(url1, url2, url3, pages, sex):\n",
    "    #Function to scrape modern virgin london marathon results page (2020 and 2019)\n",
    "    #Set up empty dataframe for results\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    #Loop through all pages to be scraped\n",
    "    for i in range(pages+1):\n",
    "        #Use requests to get content from site\n",
    "        site=requests.get(url1+str(i)+url2+sex+url3).content\n",
    "        #Soup strainer restricts content to sped up soup\n",
    "        strainer = SoupStrainer(id=\"cbox-main\")\n",
    "        #Parse the html\n",
    "        soup = BeautifulSoup(site,'lxml', parse_only=strainer)\n",
    "        \n",
    "        #Loop through each row and column to create a list of cells\n",
    "        my_table = []\n",
    "        for row in soup.find_all(class_='list-group-item'):\n",
    "            row_data = []\n",
    "            for cell in row.find_all(class_='list-field'):\n",
    "                row_data.append(cell.text)\n",
    "            \n",
    "            #If the row isn't empty, then create a dict of the row to create datafram from\n",
    "            if(len(row_data) > 0):\n",
    "                data_item = {\"Place (Overall)\": row_data[0],\n",
    "                             \"Place (Gender)\": row_data[1],\n",
    "                             \"Place (Category)\": row_data[2],\n",
    "                             \"Name\": row_data[3],\n",
    "                             \"Sex\": sex,\n",
    "                             \"Club\": row_data[4],\n",
    "                             \"Running Number\": row_data[5],\n",
    "                             \"Category\": row_data[6],\n",
    "                             \"Finish\": row_data[7],\n",
    "                }\n",
    "                my_table.append(data_item)\n",
    "        \n",
    "        #Strip table header\n",
    "        df = pd.DataFrame(my_table).iloc[1:]\n",
    "        \n",
    "        #Append to results\n",
    "        results = results.append(df)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_results_old(url1, url2, pages, sex):\n",
    "    #Function to scrape old virgin london marathon results page (2014 to 2018)\n",
    "    #Set up empty dataframe for results\n",
    "    results = pd.DataFrame()\n",
    "\n",
    "    #Loop through all pages to be scraped\n",
    "    for i in range(pages+1):\n",
    "        #Use requests to get content from site\n",
    "        site=requests.get(url1+str(i)+url2+sex).content\n",
    "        #Soup strainer restricts content to sped up soup\n",
    "        strainer = SoupStrainer('tbody')\n",
    "        #Parse the html\n",
    "        soup = BeautifulSoup(site,'lxml', parse_only=strainer)\n",
    "\n",
    "        #Loop through each row and column to create a list of cells\n",
    "        my_table = []\n",
    "        for row in soup.find_all('tr'):\n",
    "            row_data = []\n",
    "            for cell in row.find_all('td'):\n",
    "                row_data.append(cell.text)\n",
    "\n",
    "            #If the row isn't empty, then create a dict of the row to create datafram from\n",
    "            if(len(row_data) > 0):\n",
    "                data_item = {\"Place (Overall)\": row_data[0],\n",
    "                             \"Place (Gender)\": row_data[1],\n",
    "                             \"Place (Category)\": row_data[2],\n",
    "                             \"Name\": row_data[3],\n",
    "                             \"Sex\": sex,\n",
    "                             \"Club\": row_data[4],\n",
    "                             \"Running Number\": row_data[5],\n",
    "                             \"Category\": row_data[6],\n",
    "                             \"Finish\": row_data[8],\n",
    "                }\n",
    "                my_table.append(data_item)\n",
    "\n",
    "        #Strip table header\n",
    "        df = pd.DataFrame(my_table).iloc[1:]\n",
    "\n",
    "        #Append to results\n",
    "        results = results.append(df)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get results for recent 2020\n",
    "url1='https://results.virginmoneylondonmarathon.com/2020/?page='\n",
    "url2='&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D='\n",
    "url3='&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "#Get results for men, 22 pages of results <-There is no search option for other gender/sex\n",
    "mens_2020 = get_results_new(url1, url2, url3, pages=22, sex='M')\n",
    "#Get results for women, 22 pages of results\n",
    "womens_2020 = get_results_new(url1, url2, url3, pages=22, sex='W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.96 s ± 611 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "#Get results for recent 2020\n",
    "url1='https://results.virginmoneylondonmarathon.com/2020/?page='\n",
    "url2='&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D='\n",
    "url3='&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "#Get results for men, 22 pages of results <-There is no search option for other gender/sex\n",
    "mens_2020_test = get_results_new(url1, url2, url3, pages=2, sex='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get results for recent 2019\n",
    "url1='https://results.virginmoneylondonmarathon.com/2019/?page='\n",
    "url2='&event=ALL&num_results=1000&pid=search&pidp=results_nav&search%5Bsex%5D='\n",
    "url3='&search%5Bage_class%5D=%25&search%5Bnation%5D=%25&search_sort=name'\n",
    "#Get results for men, 22 pages of results <-There is no search option for other gender/sex\n",
    "mens_2019 = get_results_new(url1, url2, url3, pages=25, sex='M')\n",
    "#Get results for women, 22 pages of results\n",
    "womens_2019 = get_results_new(url1, url2, url3, pages=18, sex='W')\n",
    "\n",
    "# Concatenate results\n",
    "results_2019_2020 = pd.concat([mens_2020, womens_2020, mens_2019, womens_2019])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for 2014 to 2018, searches for number of pages\n",
    "results_2014_2018 = pd.DataFrame()\n",
    "for year in range(2014, 2019):\n",
    "    url1='https://results.virginmoneylondonmarathon.com/'+str(year)+'/?page='\n",
    "    url2='&event=MAS&num_results=1000&pid=list&search%5Bage_class%5D=%25&search%5Bsex%5D='\n",
    "    \n",
    "    site_m=requests.get(url1+'1'+url2+'M').text\n",
    "    site_w=requests.get(url1+'1'+url2+'W').text\n",
    "    soup_m = BeautifulSoup(site_m,'lxml')\n",
    "    soup_w = BeautifulSoup(site_w,'lxml')\n",
    "\n",
    "    m_pages = int(soup_m.find(class_='pages').text[-4:-2])\n",
    "    w_pages = int(soup_w.find(class_='pages').text[-4:-2])\n",
    "    \n",
    "    \n",
    "    mens = get_results_2014_2018(url1, url2, pages=m_pages, sex='M')\n",
    "    womens = get_results_2014_2018(url1, url2, pages=w_pages, sex='W')\n",
    "    \n",
    "    results_2014_2018 = results_2014_2018.append(pd.concat([mens, womens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results for 2010 to 2013\n",
    "results_2010_2013 = pd.DataFrame()\n",
    "for year in range(2010, 2014):\n",
    "    url1='https://results.virginmoneylondonmarathon.com/'+str(year)+'/index.php?page='\n",
    "    url2='&event=MAS&num_results=1000&pid=search&search%5Bsex%5D='\n",
    "    site_m=requests.get(url1+'1'+url2+'M').text\n",
    "    site_w=requests.get(url1+'1'+url2+'W').text\n",
    "    soup_m = BeautifulSoup(site_m,'lxml')\n",
    "    soup_w = BeautifulSoup(site_w,'lxml')\n",
    "\n",
    "    m_pages = int(soup_m.find(class_='pages').text[-4:-2])\n",
    "    w_pages = int(soup_w.find(class_='pages').text[-4:-2])\n",
    "    \n",
    "    \n",
    "    mens = get_results_old(url1, url2, pages=m_pages, sex='M')\n",
    "    womens = get_results_old(url1, url2, pages=w_pages, sex='W')\n",
    "    \n",
    "    results_2010_2013 = results_2010_2013.append(pd.concat([mens, womens]))\n",
    "\n",
    "london_marathon_results = pd.concat(results_2019_2020, results_2014_2018, results_2010_2013])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some quick data cleaning\n",
    "london_marathon_results['Club'] = london_marathon_results['Club'].str.replace(\"Club\", \"\", regex=False)\n",
    "london_marathon_results['Running Number'] = london_marathon_results['Running Number'].str.replace(\"Running Number\", \"\", regex=False)\n",
    "london_marathon_results['Category'] = london_marathon_results['Category'].str.replace(\"Category\", \"\", regex=False)\n",
    "london_marathon_results['Finish'] = london_marathon_results['Finish'].str.replace(\"Finish\", \"\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what we've got\n",
    "london_marathon_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And quickly save them in a csv\n",
    "london_marathon_results.to_csv(r'C:\\Users\\michael.walshe\\Documents\\Python and CAS\\London_Marathon.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}